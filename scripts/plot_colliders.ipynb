{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8dc43f1-a22e-41df-9b03-44f9a88d3af1",
   "metadata": {},
   "source": [
    "# Plotting collider Dark Matter constraints on Direct Detection planes: an example notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125473c1-e6be-4d85-a6c5-0fa3fba301a9",
   "metadata": {},
   "source": [
    "Making up 85% of the matter in the universe, **dark matter** (DM) is sought by multiple experiments, following different theoretical hypotheses. Making progress on our understanding of the nature of dark matter requires a strategy that includes a variety of complementary experiments. In this notebook, we show how (future) collider constraints are displayed in the DM-nucleon cross-section plane that is used by direct detection experiments, using a benchmark model where the interactions between dark matter and known (Standard Model, or SM) particles are mediated by a new vector particle.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafe578-378c-45c3-b8bc-8ec7d49b5cb6",
   "metadata": {},
   "source": [
    "**Credits**: This work is conducted in the context of the 2025 European Strategy Update, and is one of the projects in the initiative for Dark Matter in Europe and Beyond (https://iDMEu.org). The explanatory text in these cells is taken from [2] and [3]. This project has received funding from the European Research Council under the European Union’s Horizon 2020 research and innovation program (grant agreement 679305 and 101002463). The main authors of this notebook are Katherine Pachal (TRIUMF), Léo Chazallet (LAPP) and Caterina Doglioni (University of Manchester). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ed163-d0bf-4a0c-90c8-ca955f156298",
   "metadata": {},
   "source": [
    "## Introduction to the model and its parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751380c-4b4e-4429-9b5f-616b4d42a4b4",
   "metadata": {},
   "source": [
    "The Feynman diagram of the model we are testing for collider experiments is below. The mediator is termed as $V$ ($A-V$ represents the axial vector mediator), quarks are denoted by $q$ and gluons by $g$, while the Dark Matter particle is called $\\chi$.\n",
    "\n",
    "The relevant parameters for this model are:\n",
    "   * The coupling between quarks and the mediator $g_q$\n",
    "   * The coupling between DM and the mediator $g_{DM}$\n",
    "   * The mediator mass $m_V$\n",
    "   * The dark matter mass $m_{DM}$\n",
    "\n",
    "The model also allows for a single universal coupling strength $g_l$ for all mediator-lepton interactions, including neutrinos (this is not depicted in the diagrams below). No mixing with the Z-boson is included in this model.\n",
    "\n",
    "While this is a _simplified model_, meaning that not all of its features are fully developed at all scales, portraying the sensitivities of different experiments using a common benchmark is still useful to understand how DM would manifest across experiments in similar classes of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfaec3-3e6f-4300-abae-bd8827cc18a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename = \"img/Feynman.png\", width=600, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa880c-7d93-41fe-8743-f1b9a3aed38c",
   "metadata": {},
   "source": [
    "## Searches at colliders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82aa9a-0301-4625-a667-40a8702a7dbf",
   "metadata": {},
   "source": [
    "Due to the presence of both SM and DM couplings, the main search avenues for this kind of models at colliders are (a, left-hand side diagram) by looking for localized (resonant) excesses signaling the presence of a mediator decaying into pairs of DM particles and (b, right-hand side diagram) by looking for an excess of missing energy (MET) from the mediator decaying to DM particles.  \n",
    "\n",
    "The inputs received from the collider community in the context of Snowmass 2021 and used to prepare the plots in this notebook are: \n",
    "   * MET+jet search (also called monojet search) using the ATLAS detector at the upgraded LHC (HL-LHC) [hllhc-monojet];\n",
    "   * MET+jet and MET+hadronically decaying vector boson at the Future Hadron Collider in the Future Circular Collider (FCC) complex [Harris:2015kda];\n",
    "   * Dijet resonance search at the HL-LHC and at the Future Hadron Collider [Harris:2022kls];\n",
    "   * Dilepton resonance search at the HL-LHC using the ATLAS detector [ATL-PHYS-PUB-2018-044]. The CMS version of this search is also available at [CMS-PAS-FTR-21-005]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefb4a1-27ca-4ad8-aca7-6941eac1361d",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df6cd8e-0737-46fd-ae3a-206da4c2c9bb",
   "metadata": {},
   "source": [
    "[Explain that we need more than one experiment to make a discovery] \n",
    "\n",
    "So far, the presentation of LHC and future hadron collider results has focused on four benchmark scenarios with specific coupling values within these simplified models - in particular, one of the chosen benchmarks (also used in the 2019 European Strategy Briefing Book [4]) use g_q = 0.25, g_l = 0 and g_DM = 1.0. [The following is a WIP] In this work, we highlight the extensions of this benchmark scenario to arbitrary couplings and show how the areas of complementarity (where both direct detection and collider experiments could share information about a discovery) change under different coupling scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c39c4-de2d-4791-872c-eb846bfd56a9",
   "metadata": {},
   "source": [
    "## Notebook for overlaying the sensitivity of future collider and direct detection experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbdf86b-ed4b-4519-87a0-79ad9e52055c",
   "metadata": {},
   "source": [
    "### Common imports and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35a6bb-d523-408e-b812-810c6cc66b17",
   "metadata": {},
   "source": [
    "Assorted imports, including a helper script to collect and display direct detection curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b31df7-44e0-4436-8449-550b54ba180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from helpers import pairwise\n",
    "from shapely.geometry import Polygon as shapely_pol\n",
    "import pickle\n",
    "#from basic_plotter import *\n",
    "import ROOT\n",
    "import itertools as it\n",
    "import sys, os\n",
    "sys.path.insert(1, '../inputs/directdetection')\n",
    "import collect_dd\n",
    "\n",
    "FULL_SPECTRA_PATH = \"data/Full_spectra\"  # arXiv:[2109.03116]\n",
    "OUTPUT_FOLDER = \"plots\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449f390-f8e9-4bb5-b23c-11316029dd1b",
   "metadata": {},
   "source": [
    "Some more helper functions (TODO: move outside this notebook at some point...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1fd17-9aa7-4cfa-adcf-9fd1a0101cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_vertical(x1,y1,x2,y2,n) :\n",
    "    ov = sorted([[y1,x1],[y2,x2]])\n",
    "    new_ys = np.linspace(ov[0][0], ov[1][0], num=n)\n",
    "    new_xs = np.interp(new_ys,[ov[0][0],ov[1][0]],[ov[0][1],ov[1][1]])\n",
    "    vertices = list(zip(new_xs,new_ys))\n",
    "    return vertices if y1 < y2 else list(reversed(vertices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290cd9ee-160c-4bdd-be12-4f4983a5dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    # pairwise('ABCDEFG') --> AB BC CD DE EF FG\n",
    "    a, b = it.tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e43118-9a91-4192-8904-fca165f8c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect direct detection contours for comparison plots.\n",
    "def get_dd_lines(lineinfo) :\n",
    "    legend_lines = list(lineinfo.keys())\n",
    "    dd_lines = []\n",
    "    for name in legend_lines :\n",
    "        dd_lines.append(lineinfo[name])\n",
    "    return legend_lines, dd_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1efa39a-833c-4512-b0f0-e3f0edb58385",
   "metadata": {},
   "source": [
    "### Plotting collider curves - WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35092914-b198-4493-b011-054bcd5b7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ColorConverter#,ListedColormap, LinearSegmentedColormap,rgb2hex\n",
    "import matplotlib.colors as cols\n",
    "from matplotlib.patches import Polygon\n",
    "from shapely.ops import unary_union, polygonize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786d738-631c-401a-9954-f92c10465021",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_proton = collect_dd.get_sd_proton()\n",
    "sd_neutron = collect_dd.get_sd_neutron()\n",
    "spin_independent = collect_dd.get_spin_independent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6305de-640f-4a9f-a332-4c7e06ef27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gq = [0.25]\n",
    "test_gdm = [1.0]\n",
    "test_gl = [0.00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c00979-f9bd-47fb-b8c5-a883df308ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_ratio(ax, isloglog=False, islinearlog=False) :\n",
    "  ratio = 1.0\n",
    "  xleft, xright = ax.get_xlim()\n",
    "  ybottom, ytop = ax.get_ylim()\n",
    "  if isloglog :\n",
    "    nUnitsX = np.log10(xright) - np.log10(xleft)\n",
    "    nUnitsY = np.log10(ytop) - np.log10(ybottom)\n",
    "    return nUnitsX/nUnitsY\n",
    "  elif islinearlog :\n",
    "    nUnitsY = np.log10(ytop) - np.log10(ybottom)\n",
    "    nUnitsX = (xright-xleft)\n",
    "    return abs(nUnitsX/nUnitsY)\n",
    "  else :\n",
    "    return abs((xright-xleft)/(ybottom-ytop))*ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d042e2e-34ae-4947-a40b-0b09c47f00db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eqn 4.10 https://arxiv.org/pdf/1603.04156.pdf#page12\n",
    "def calculate_sd(gq, gdm, gl, mMed, mdm) :\n",
    "\n",
    "    mn = 0.939 # GeV\n",
    "    val = 2.4e-42 * (gq*gdm/0.25)**2 * (1000./mMed)**4 * (mn*mdm/(mn+mdm))**2\n",
    "    return val\n",
    "\n",
    "# eqn 4.3: https://arxiv.org/pdf/1603.04156.pdf#page12\n",
    "def calculate_si(gq, gdm, gl, mMed, mdm) :\n",
    "\n",
    "    mn = 0.939 # GeV\n",
    "    val = 6.9e-41 * (gq*gdm/0.25)**2 * (1000./mMed)**4 * (mn*mdm/(mn+mdm))**2\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccae6d0-6b69-4407-9a7e-abc939f51b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_exclusions(contour_list) :\n",
    "\n",
    "    # If there's only one, do nothing.\n",
    "    if len(contour_list) == 1 :\n",
    "      return contour_list[0]\n",
    "    elif len(contour_list) == 0 :\n",
    "      return None\n",
    "\n",
    "    # No longer matters which input each contour is from:\n",
    "    # flatten the input to a simple list of polygons\n",
    "    contour_list = [item for sublist in contour_list for item in sublist]\n",
    "\n",
    "    # Merge\n",
    "    merged = unary_union(contour_list)\n",
    "\n",
    "    # Split multipolygon back into list of polygons, if needed\n",
    "    poly_list = []\n",
    "    if merged.geom_type == 'MultiPolygon':\n",
    "      poly_list = [poly for poly in merged.geoms]\n",
    "    else :\n",
    "      poly_list = [merged]\n",
    "\n",
    "    return poly_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ad767-e7d3-4628-bfe0-02a9d8b2ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorFader_collider_only(c1,c2,mix=0): #fade (linear interpolate) from color c1 (at mix=0) to c2 (mix=1)\n",
    "    c1=np.array(ColorConverter.to_rgb(c1))\n",
    "    c2=np.array(ColorConverter.to_rgb(c2))\n",
    "    return cols.to_hex((1-mix)*c1 + mix*c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c067f5-c3a5-4792-b783-21735e4793c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coreDrawFunction_collider_only(axes,contour_groups,legend_lines,addText = \"\",is_scaling=False, transluscent=False,xlow=None,xhigh=None, ylow=None,yhigh=None, use_colourscheme=False, dash_contour = [],gradient_fill=[],dashed_lines=[], dashed_legends=[],text_spot = 0,logx=False,logy=False,dd_curves = None, dd_legendlines = None) :\n",
    "\n",
    "    # Check input\n",
    "    if not dash_contour : \n",
    "      dash_contour = [False for i in contour_groups]\n",
    "    elif len(dash_contour) != len(contour_groups) :\n",
    "      print(\"Error: dash_contour list must have an entry for each contour!\")\n",
    "      exit(1)\n",
    "    if not gradient_fill :\n",
    "      gradient_fill = [False for i in contour_groups]\n",
    "    elif len(gradient_fill) != len(contour_groups) :\n",
    "      print(\"Error: gradient_fill list must have an entry for each contour!\")    \n",
    "      exit(1)\n",
    "\n",
    "    if addText :\n",
    "        text_x = 0.25 if not text_spot else text_spot[0]\n",
    "        text_y = 0.82 if not text_spot else text_spot[1]\n",
    "        text_ploty = text_y - 0.05*addText.count('\\n')\n",
    "        plt.figtext(text_x,text_ploty,addText,size=14)\n",
    "\n",
    "    # Need cute colours\n",
    "    if is_scaling :\n",
    "        ncols = len(contour_groups)\n",
    "        ncols_dashed = len(dashed_lines)\n",
    "        # old version: all blues. Great for overlaid but hard to tell apart if transluscent.\n",
    "        #fill_colours = [scale_lightness('cornflowerblue',0.5+i/(ncols-1) for i in range(ncols)]\n",
    "        if ncols < 2 :\n",
    "          colours_raw = [colorFader_collider_only('cornflowerblue','turquoise',0.5)] # other good option: 'lightgreen'\n",
    "        else :\n",
    "          colours_raw = [colorFader_collider_only('cornflowerblue','turquoise',i/(ncols-1)) for i in range(ncols)]\n",
    "        # Additional dashed line colours: want contrast if we're doing scaling\n",
    "        if ncols_dashed < 2 :\n",
    "          colours_dashed = [colorFader_collider_only('crimson','gold',0.5)]\n",
    "        else :\n",
    "          colours_dashed = [colorFader_collider_only('crimson','gold',i/(ncols-1)) for i in range(ncols)]\n",
    "        if transluscent :\n",
    "          fill_colours = [ColorConverter.to_rgba(col, alpha=0.5) for col in colours_raw]\n",
    "          line_colours = ['black' for i in fill_colours]\n",
    "        else :\n",
    "          line_colours = colours_raw\n",
    "        line_width = 1\n",
    "    else :\n",
    "        if len(contour_groups) < 4 or use_colourscheme:\n",
    "          colours_raw = ['cornflowerblue','turquoise','mediumorchid']\n",
    "          colours_dashed = ['royalblue','darkcyan','darkorchid']\n",
    "        else :\n",
    "          from matplotlib.pyplot import cm\n",
    "          colours_raw = cm.rainbow(np.linspace(0, 1, len(contour_groups)))\n",
    "          colours_dashed = colours_raw\n",
    "        fillOpacity = 0.5\n",
    "        fill_colours = [ColorConverter.to_rgba(col, alpha=fillOpacity) for col in colours_raw]\n",
    "        line_colours = colours_raw\n",
    "        line_width = 2\n",
    "    # Going to do a colour scheme for clarity\n",
    "    for group_index, (contour_group,label_line, dashed, gradient) in enumerate(zip(contour_groups,legend_lines,dash_contour,gradient_fill)) :\n",
    "        if use_colourscheme :\n",
    "          if \"dijet\" in label_line or \"Dijet\" in label_line : icol = 0\n",
    "          elif \"mono\" in label_line or \"Mono\" in label_line : icol = 1\n",
    "          else : icol = 2\n",
    "        else :\n",
    "          icol = group_index\n",
    "        for index, contour in enumerate(contour_group) :\n",
    "          if index == 0 : uselabel = label_line\n",
    "          else : uselabel = \"_\"\n",
    "          if dashed :\n",
    "            #usefacecolor = 'none'\n",
    "            uselinecolor = colours_dashed[icol]\n",
    "            #uselinecolor = line_colours[icol]\n",
    "            linestyle='dashed'\n",
    "          else :\n",
    "            uselinecolor = line_colours[icol]\n",
    "            linestyle='solid'\n",
    "          if gradient :\n",
    "            im = get_gradient_fill(axes,contour,fill_colours[icol],logx,logy) # colours_raw\n",
    "            usefacecolor = 'none'\n",
    "            # Need to mess with legend with an un-plotted patch.\n",
    "            # This is hacky but whatever.\n",
    "            if index == 0 :\n",
    "              patch_forleg = Polygon([[-2,0],[-1,0],[-1,-1]], facecolor=fill_colours[icol], edgecolor=uselinecolor, label=uselabel,linewidth=line_width,linestyle=linestyle)\n",
    "              axes.add_patch(patch_forleg)\n",
    "              uselabel = \"_\"\n",
    "          else :\n",
    "            usefacecolor = fill_colours[icol]\n",
    "          # Drawing here\n",
    "          if len(list(contour.exterior.coords)) < 3 :\n",
    "            continue\n",
    "          patch = Polygon(list(contour.exterior.coords), facecolor=usefacecolor, edgecolor=uselinecolor, zorder=2, label=uselabel,linewidth=line_width,linestyle=linestyle) \n",
    "          axes.add_patch(patch)\n",
    "          if gradient : im.set_clip_path(patch)\n",
    "      \n",
    "    if dashed_lines :\n",
    "      for i,(newline,label) in enumerate(zip(dashed_lines,dashed_legends)) :\n",
    "        #plt.plot(newline[0],newline[1], color=dd_colours[i]) # for an actual line\n",
    "        for j,line in enumerate(newline) :\n",
    "          patch = Polygon(list(line.exterior.coords),facecolor='none',edgecolor=colours_dashed[i],label=(label if j==0 else \"_\"),zorder=2,linewidth=2,linestyle='dashed')\n",
    "          axes.add_patch(patch)\n",
    "\n",
    "    if dd_curves :\n",
    "      dd_colours = ['crimson','darkorange','gold','deeppink']\n",
    "      for i,(newline,label) in enumerate(zip(dd_curves,dd_legendlines)) :\n",
    "        plt.plot(newline[0],newline[1], color=dd_colours[i], label=label.replace(\" \",\"\\n\"))      \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73ec9f-2cf1-4b95-b3ef-25a8512e713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawPlot_collider_only(contour_groups, legend_lines, this_tag = \"default\", plot_path = \"plots_collider_only\", addText = \"\",ylabel=\"$\\sigma$\",is_scaling=False, transluscent=False, xlow=None, xhigh=None, ylow=None,yhigh=None, dd_curves = None, dd_legendlines = None,dashed_lines=[], dashed_legends=[]) :\n",
    "\n",
    "    # Check output\n",
    "    if not os.path.exists(plot_path) :\n",
    "        os.makedirs(plot_path)\n",
    "\n",
    "    # Object for plotting\n",
    "    fig,ax=plt.subplots(1,1)\n",
    "\n",
    "    usexhigh = xhigh if xhigh else 2000\n",
    "    useylow = ylow if ylow else 1e-46\n",
    "    useyhigh = yhigh if yhigh else 1e-37\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')    \n",
    "    ax.set_xlim(1, usexhigh)\n",
    "    ax.set_ylim(useylow,useyhigh)\n",
    "    ratio = get_aspect_ratio(ax, isloglog=True)\n",
    "    ax.set_aspect(ratio)\n",
    "    plt.rc('font',size=16)\n",
    "\n",
    "    ax.set_xlabel(\"m$_{DM}$ [GeV]\", fontsize=16)\n",
    "    ax.set_ylabel(ylabel, fontsize=16)\n",
    "\n",
    "    text_spot = [0.84, 0.83]\n",
    "    coreDrawFunction_collider_only(ax,contour_groups,legend_lines,addText,is_scaling, transluscent,xlow,xhigh, ylow,yhigh, use_colourscheme=False, dash_contour=[],gradient_fill=[],dashed_lines=dashed_lines,dashed_legends=dashed_legends,text_spot=text_spot,logx=True,logy=True,dd_curves = dd_curves, dd_legendlines = dd_legendlines)\n",
    "\n",
    "    leg_y = text_spot[1]-0.05*(addText.count('\\n')-2)\n",
    "    leg = ax.legend(fontsize=14,bbox_to_anchor=(1.02,leg_y),loc=\"upper left\")\n",
    "    leg.get_frame().set_linewidth(0.0)\n",
    "\n",
    "    plt.savefig(plot_path+'/{0}.pdf'.format(this_tag),bbox_inches='tight')\n",
    "\n",
    "    plt.close(fig)    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3e4b7-453c-46fc-9592-9ce26aaec052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots_collider_only(collider, model, contours, legend_lines, fix_couplings, extra_tag = \"\") :\n",
    "    xlow = 1\n",
    "    xhigh = 2000 if 'hl-lhc' in collider else 4000\n",
    "    if 'vector' in model : ylow = 1e-48 if 'hl-lhc' in collider else 1e-50\n",
    "    else : ylow = 1e-46 if 'hl-lhc' in collider else 1e-50\n",
    "    yhigh = 1e-37 if 'hl-lhc' in collider else 1e-42    \n",
    "    usepath = \"plots/collider_only/\"+collider\n",
    "    formatters = {\"gq\" : \"q\", \"gdm\" : \"DM\", \"gl\" : \"l\"}\n",
    "    treat_as_scaling = False\n",
    "    # At least 2 fixed couplings.\n",
    "    if len(fix_couplings.keys()) > 2 :\n",
    "        label_line =  \"{0}\\n{7}, g$_{5}$={2}\\ng$_{4}$={1}, g$_{6}$={3}\".format((\"Axial-vector\" if 'axial' in model else \"Vector\"),fix_couplings[\"gq\"],fix_couplings[\"gdm\"],fix_couplings[\"gl\"],\"q\",\"DM\",\"l\",collider.upper())\n",
    "        tag_line = model+\"_gq{0}_gdm{1}_gl{2}\".format(fix_couplings[\"gq\"],fix_couplings[\"gdm\"],fix_couplings[\"gl\"])\n",
    "    else :\n",
    "        treat_as_scaling = True\n",
    "        usecouplings = list(fix_couplings.keys())\n",
    "        useformats = []\n",
    "        vals = []\n",
    "        for coupling in usecouplings :\n",
    "            useformats.append(formatters[coupling])\n",
    "            vals.append(fix_couplings[coupling])\n",
    "        label_line = \"{0}, {5}\\ng$_{3}$={1}, g$_{4}$={2}\".format((\"Axial-vector\" if 'axial' in model else \"Vector\"),vals[0],vals[1],useformats[0],useformats[1],collider.upper())\n",
    "        tag_line = model+\"_{0}{1}_{2}{3}\".format(usecouplings[0],vals[0],usecouplings[1],vals[1])\n",
    "    if extra_tag : tag_line = tag_line + \"_\" + extra_tag\n",
    "    # And draw. First, version without DD experiment lines\n",
    "    # Then draw the plots with DD lines on\n",
    "    if 'vector' in model :\n",
    "        use_ylabel = r\"$\\sigma_{SI}$ ($DM$-nucleon) [cm$^2$]\"\n",
    "        drawPlot_collider_only(contours,legend_lines, this_tag = tag_line, plot_path = usepath, addText = label_line, ylabel=use_ylabel, is_scaling=treat_as_scaling, transluscent=treat_as_scaling, xhigh=xhigh, ylow=ylow, yhigh=yhigh)\n",
    "    else :\n",
    "        use_ylabel = r\"$\\sigma_{SD}$ ($DM$-nucleon) [cm$^2$]\"\n",
    "        drawPlot_collider_only(contours,legend_lines, this_tag = tag_line, plot_path = usepath, addText = label_line, ylabel=use_ylabel, is_scaling=treat_as_scaling, transluscent=treat_as_scaling, xhigh=xhigh, ylow=ylow, yhigh=yhigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf45f69-16ca-406f-baf4-b5733988d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-level driver function\n",
    "def load_polygons(model, collider):\n",
    "    with open(f\"{model}_exclusion_contours_{collider}.pkl\", \"rb\") as poly_file:\n",
    "        return pickle.load(poly_file)\n",
    "\n",
    "def process_signature(signature, gq, gdm, gl, loaded_polygons, model):\n",
    "    if (gq, gdm, gl) not in loaded_polygons[signature]:\n",
    "        return None, None\n",
    "    exclusions = loaded_polygons[signature][(gq, gdm, gl)]\n",
    "    if not exclusions:\n",
    "        return None, None\n",
    "\n",
    "    inner_contours = []\n",
    "    for contour in exclusions:\n",
    "        original_vertices = list(contour.exterior.coords)\n",
    "        use_vertices = []\n",
    "        for (x, y), (xnext, ynext) in pairwise(original_vertices + [original_vertices[0]]):\n",
    "            use_vertices.append((x, y))\n",
    "            if (y > 0 or ynext > 0) and ynext < 50:\n",
    "                use_vertices.extend(interpolate_vertical(x, y, xnext, ynext, 100))\n",
    "        contour_DD_raw = []\n",
    "        raw_graph = ROOT.TGraph()\n",
    "        for (x, y) in use_vertices:\n",
    "            sigma = calculate_sd(gq, gdm, gl, x, y) if model == 'axial' else calculate_si(gq, gdm, gl, x, y)\n",
    "            contour_DD_raw.append((y, sigma))\n",
    "            raw_graph.AddPoint(y, sigma)\n",
    "        print(contour_DD_raw)\n",
    "        contour_DD = shapely_pol(contour_DD_raw)\n",
    "        inner_contours.append(contour_DD)\n",
    "    return inner_contours, signature.capitalize()\n",
    "\n",
    "def generate_exclusions(test_gq, test_gdm, test_gl, loaded_polygons, model, collider):\n",
    "    exclusions_dd = {'dijet': {}, 'monojet': {}, 'dilepton': {}}\n",
    "    exclusions_separate_dd = {'dijet': {}, 'monojet': {}, 'dilepton': {}}\n",
    "\n",
    "    for gdm, gq, gl in it.product(test_gdm, test_gq, test_gl):\n",
    "        contours_list = []\n",
    "        legend_lines = []\n",
    "        for signature in ['dijet', 'monojet', 'dilepton']:\n",
    "            inner_contours, legend = process_signature(signature, gq, gdm, gl, loaded_polygons, model)\n",
    "            if inner_contours is None:\n",
    "                continue\n",
    "            contours_list.append(inner_contours)\n",
    "            legend_lines.append(legend)\n",
    "            exclusions_dd[signature][(gq, gdm, gl)] = contours_list\n",
    "            exclusions_separate_dd[signature][(gq, gdm, gl)] = inner_contours\n",
    "        make_plots_collider_only(collider, model, contours_list, legend_lines, {\"gq\": gq, \"gdm\": gdm, \"gl\": gl})\n",
    "    return exclusions_dd, exclusions_separate_dd\n",
    "\n",
    "def generate_combined_plots(test_gq, test_gdm, test_gl, exclusions_dd, exclusions_separate_dd, collider, model):\n",
    "    for gdm in test_gdm:\n",
    "        for gl in test_gl:\n",
    "            contours_list_couplingscan = []\n",
    "            legend_lines_couplingscan = []\n",
    "            for gq in test_gq:\n",
    "                contours = [exclusions_dd[sig].get((gq, gdm, gl), []) for sig in ['dijet', 'monojet', 'dilepton']]\n",
    "                merged = merge_exclusions([c for sublist in contours for c in sublist])\n",
    "                if not merged:\n",
    "                    continue\n",
    "                contours_list_couplingscan.append(merged)\n",
    "                legend_lines_couplingscan.append(f\"g$_{{q}}$={gq}\")\n",
    "            make_plots_collider_only(collider, model, contours_list_couplingscan, legend_lines_couplingscan, {\"gl\": gl, \"gdm\": gdm})\n",
    "\n",
    "    for gq in test_gq:\n",
    "        for gl in test_gl:\n",
    "            # First grouped loop\n",
    "            contours_list_couplingscan = []\n",
    "            legend_lines_couplingscan = []\n",
    "            for gdm in test_gdm:\n",
    "                contours = [exclusions_dd[sig].get((gq, gdm, gl), []) for sig in ['dijet', 'monojet', 'dilepton']]\n",
    "                if all(not i for i in contours):\n",
    "                    continue\n",
    "                merged = merge_exclusions([c for sublist in contours for c in sublist])\n",
    "                contours_list_couplingscan.append(merged)\n",
    "                legend_lines_couplingscan.append(f\"g$_{{\\chi}}$={gdm}\")\n",
    "            make_plots_collider_only(collider, model, contours_list_couplingscan, legend_lines_couplingscan, {\"gq\": gq, \"gl\": gl})\n",
    "        \n",
    "            # Second grouped loop\n",
    "            for signature in ['dijet', 'monojet', 'dilepton']:\n",
    "                sub_contours_list = []\n",
    "                sub_legends_list = []\n",
    "                for gdm in test_gdm:\n",
    "                    if (gq, gdm, gl) in exclusions_separate_dd[signature]:\n",
    "                        sub_contours_list.append(exclusions_separate_dd[signature][(gq, gdm, gl)])\n",
    "                        sub_legends_list.append(f\"g$_{{\\chi}}$={gdm}\")\n",
    "                make_plots_collider_only(collider, model, sub_contours_list, sub_legends_list, {\"gq\": gq, \"gl\": gl}, extra_tag=signature)\n",
    "    #         contours_list_couplingscan = []\n",
    "    #         legend_lines_couplingscan = []\n",
    "    #         for gdm in test_gdm:\n",
    "    #             contours = [exclusions_dd[sig].get((gq, gdm, gl), []) for sig in ['dijet', 'monojet', 'dilepton']]\n",
    "    #             if all(not i for i in contours):\n",
    "    #                 continue\n",
    "    #             merged = merge_exclusions([c for sublist in contours for c in sublist])\n",
    "    #             contours_list_couplingscan.append(merged)\n",
    "    #             legend_lines_couplingscan.append(f\"g$_{{\\chi}}$={gdm}\")\n",
    "    #         make_plots_collider_only(collider, model, contours_list_couplingscan, legend_lines_couplingscan, {\"gq\": gq, \"gl\": gl})\n",
    "\n",
    "    # for gq, gl in it.product(test_gq, test_gl):\n",
    "    #     for signature in ['dijet', 'monojet', 'dilepton']:\n",
    "    #         sub_contours_list = []\n",
    "    #         sub_legends_list = []\n",
    "    #         for gdm in test_gdm:\n",
    "    #             if (gq, gdm, gl) in exclusions_separate_dd[signature]:\n",
    "    #                 sub_contours_list.append(exclusions_separate_dd[signature][(gq, gdm, gl)])\n",
    "    #                 sub_legends_list.append(f\"g$_{{\\chi}}$={gdm}\")\n",
    "    #         make_plots_collider_only(collider, model, sub_contours_list, sub_legends_list, {\"gq\": gq, \"gl\": gl}, extra_tag=signature)\n",
    "\n",
    "for collider in ['hl-lhc']:  # , 'fcc-hh'\n",
    "    for model in ['vector', 'axial']:\n",
    "        # Load pickle files with polygons\n",
    "        # Limits with fixed couplings\n",
    "        loaded_polygons = load_polygons(model, collider)\n",
    "        exclusions_dd, exclusions_separate_dd = generate_exclusions(test_gq, test_gdm, test_gl, loaded_polygons, model, collider)\n",
    "        generate_combined_plots(test_gq, test_gdm, test_gl, exclusions_dd, exclusions_separate_dd, collider, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b032a-4c14-44e2-b18a-c4ee7c597fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle files with polygons\n",
    "# Limits with fixed couplings\n",
    "for collider in ['hl-lhc'] :#, 'fcc-hh'] :\n",
    "    for model in ['vector','axial'] :\n",
    "\n",
    "        with open('{0}_exclusion_contours_{1}.pkl'.format(model,collider), \"rb\") as poly_file:\n",
    "            loaded_polygons = pickle.load(poly_file)\n",
    "            \n",
    "            # Grid of plots:\n",
    "            exclusions_dd = {'dijet' : {},'monojet' : {},'dilepton' : {}}\n",
    "            exclusions_separate_dd = {'dijet' : {},'monojet' : {},'dilepton' : {}}\n",
    "     \n",
    "            for gdm in test_gdm :\n",
    "                for gq in test_gq :\n",
    "                    contours_list_couplingscan = []\n",
    "                    legend_lines_couplingscan = []                \n",
    "                    for gl in test_gl :\n",
    "                        contours_list = []\n",
    "                        legend_lines = []\n",
    "                        for signature in ['dijet','monojet','dilepton'] :\n",
    "                            # e.g. no coupling to leptons, skip:\n",
    "                            if (gq, gdm, gl) not in loaded_polygons[signature].keys() :\n",
    "                                continue\n",
    "                            exclusions = loaded_polygons[signature][(gq, gdm, gl)]\n",
    "                            if not exclusions : continue\n",
    "                            inner_contours = []\n",
    "                            for contour in exclusions :\n",
    "                                original_vertices = list(contour.exterior.coords)\n",
    "                                use_vertices = []\n",
    "                                # Reconnect loop by adding first one back\n",
    "                                for (x, y), (xnext,ynext) in pairwise(original_vertices+[original_vertices[0]]):\n",
    "                                    use_vertices.append((x, y))\n",
    "                                    if (y > 0 or ynext > 0) and ynext < 50 :\n",
    "                                        new_vertices = interpolate_vertical(x,y,xnext,ynext,100)\n",
    "                                        use_vertices = use_vertices + new_vertices\n",
    "                                contour_DD_raw = []\n",
    "                                raw_graph = ROOT.TGraph()\n",
    "                                for (x, y) in use_vertices :\n",
    "                                    if model=='axial' :\n",
    "                                        sigma = calculate_sd(gq, gdm, gl, x, y)\n",
    "                                    else :\n",
    "                                        sigma = calculate_si(gq, gdm, gl, x, y)\n",
    "                                    contour_DD_raw.append((y, sigma))\n",
    "                                    raw_graph.AddPoint(y, sigma)\n",
    "                                ## For anyone wishing to take out the individual curve after merging the limits, it's here: \n",
    "                                print(contour_DD_raw)\n",
    "                                contour_DD = shapely_pol(contour_DD_raw)\n",
    "                                inner_contours.append(contour_DD)\n",
    "                            contours_list.append(inner_contours)\n",
    "                            legend_lines.append(signature.capitalize())\n",
    "                            exclusions_dd[signature][(gq, gdm, gl)] = contours_list\n",
    "                            exclusions_separate_dd[signature][(gq, gdm, gl)] = inner_contours\n",
    "                        # First set of plots: 3 contours, one plot for every coupling combo\n",
    "                        make_plots_collider_only(collider, model, contours_list, legend_lines, {\"gq\" : gq, \"gdm\" : gdm, \"gl\" : gl})\n",
    "                        full_polygons = merge_exclusions(contours_list)\n",
    "                        if not full_polygons : continue \n",
    "                        contours_list_couplingscan.append(full_polygons)\n",
    "                        legend_lines_couplingscan.append(\"g$_{0}$={1}\".format(\"l\",gl))\n",
    "                    # Second set of plots: merge all contours; fix gq and vary gl.\n",
    "                    # Note this is not meaningful where we don't have dilepton projections - skip then.\n",
    "                    make_plots_collider_only(collider, model, contours_list_couplingscan, legend_lines_couplingscan, {\"gq\" : gq, \"gdm\" : gdm})\n",
    "                    # Could do signature only, fixed gq and varying gl, \n",
    "                    # but don't think we need it right now. Would go here.\n",
    "    \n",
    "                # Need second set of plots with gl fixed instead:\n",
    "                for gl in test_gl :\n",
    "                    contours_list_couplingscan = []\n",
    "                    legend_lines_couplingscan = []\n",
    "                    for gq in test_gq :\n",
    "                        contours_list = []\n",
    "                        for signature in ['dijet','monojet','dilepton'] :\n",
    "                            # e.g. no coupling to leptons, skip:\n",
    "                            if (gq, gdm, gl) not in exclusions_dd[signature].keys() :\n",
    "                                continue\n",
    "                            exclusions = exclusions_dd[signature][(gq, gdm, gl)]\n",
    "                            contours_list+=exclusions\n",
    "                        full_polygons = merge_exclusions(contours_list)\n",
    "                        if not full_polygons : continue\n",
    "                        contours_list_couplingscan.append(full_polygons)\n",
    "                        legend_lines_couplingscan.append(\"g$_{0}$={1}\".format(\"q\",gq))\n",
    "                    make_plots_collider_only(collider, model, contours_list_couplingscan, legend_lines_couplingscan, {\"gl\" : gl, \"gdm\" : gdm})\n",
    "                    \n",
    "                    # A version overlaying all monojet and overlaying all dijet, but not combining\n",
    "                    for signature in ['dijet','monojet','dilepton'] :\n",
    "                        sub_contours_list = []\n",
    "                        sub_legends_list = []\n",
    "                        for gq in test_gq :\n",
    "                            if (gq, gdm, gl) not in exclusions_separate_dd[signature].keys() : continue\n",
    "                            sub_contours_list.append(exclusions_separate_dd[signature][(gq, gdm, gl)])\n",
    "                            sub_legends_list.append(\"g$_{0}$={1}\".format(\"q\",gq))\n",
    "                        make_plots_collider_only(collider, model, sub_contours_list, sub_legends_list, {\"gl\" : gl, \"gdm\" : gdm}, extra_tag = signature)\n",
    "            # And now third set of plots with gq and gl fixed:\n",
    "            for gq in test_gq :\n",
    "                for gl in test_gl :\n",
    "                    contours_list_couplingscan = []\n",
    "                    legend_lines_couplingscan = []\n",
    "                    for gdm in test_gdm :\n",
    "                        contours_list = []\n",
    "                        for signature in ['dijet','monojet','dilepton'] :\n",
    "                            # e.g. no coupling to leptons, skip:\n",
    "                            if (gq, gdm, gl) not in exclusions_dd[signature].keys() :\n",
    "                                continue                            \n",
    "                            exclusions = exclusions_dd[signature][(gq, gdm, gl)]\n",
    "                            contours_list+=exclusions\n",
    "                        if all(not i for i in contours_list) : continue\n",
    "                        full_polygons = merge_exclusions(contours_list)\n",
    "                        contours_list_couplingscan.append(full_polygons)\n",
    "                        legend_lines_couplingscan.append(\"g$_{0}$={1}\".format(\"\\chi\",gdm))\n",
    "                    make_plots_collider_only(collider, model, contours_list_couplingscan, legend_lines_couplingscan, {\"gq\" : gq, \"gl\" : gl})\n",
    "                    # A version overlaying all monojet and overlaying all dijet, but not combining\n",
    "                    for signature in ['dijet','monojet','dilepton'] :\n",
    "                        sub_contours_list = []\n",
    "                        sub_legends_list = []\n",
    "                        for gdm in test_gdm :\n",
    "                            if (gq, gdm, gl) not in exclusions_separate_dd[signature].keys() : continue\n",
    "                            sub_contours_list.append(exclusions_separate_dd[signature][(gq, gdm, gl)])\n",
    "                            sub_legends_list.append(\"g$_{0}$={1}\".format(\"\\chi\",gdm))                   \n",
    "                        make_plots_collider_only(collider, model, sub_contours_list,sub_legends_list, {\"gq\" : gq, \"gl\" : gl}, extra_tag = signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80e21d-41b3-4f51-a08a-8382cad4de68",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[1] A. Boveia et al., Dark Matter Benchmark Models for Early LHC Run-2 Searches: Report of the ATLAS/CMS Dark Matter Forum, https://doi.org/10.1016/j.dark.2019.100371 \n",
    "\n",
    "[2] https://arxiv.org/pdf/2203.12035\n",
    "\n",
    "[3] \n",
    "\n",
    "[4] https://arxiv.org/pdf/1603.04156#page12\n",
    "\n",
    "[5] https://arxiv.org/abs/1411.3342\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
