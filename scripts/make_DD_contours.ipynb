{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8dc43f1-a22e-41df-9b03-44f9a88d3af1",
   "metadata": {},
   "source": [
    "# Overlaying collider and Direct Detection Dark Matter constraints: an example notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125473c1-e6be-4d85-a6c5-0fa3fba301a9",
   "metadata": {},
   "source": [
    "Making up 85% of the matter in the universe, **dark matter** (DM) is sought by multiple experiments, following different theoretical hypotheses. Making progress on our understanding of the nature of dark matter requires a strategy that includes a variety of complementary experiments. In this notebook, we highlight the complementarity between two different classes of experiments, those at future colliders and direct detection experiments, using a benchmark model where the interactions between dark matter and known (Standard Model, or SM) particles are mediated by a new vector particle.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafe578-378c-45c3-b8bc-8ec7d49b5cb6",
   "metadata": {},
   "source": [
    "**Credits**: This work is conducted in the context of the 2025 European Strategy Update, and is one of the projects in the initiative for Dark Matter in Europe and Beyond (https://iDMEu.org). The explanatory text in these cells is taken from [2] and [3]. This project has received funding from the European Research Council under the European Union’s Horizon 2020 research and innovation program (grant agreement 679305 and 101002463). The main authors of this notebook are Katherine Pachal (TRIUMF), Léo Chazallet (LAPP) and Caterina Doglioni (University of Manchester). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ed163-d0bf-4a0c-90c8-ca955f156298",
   "metadata": {},
   "source": [
    "## Introduction to the model and its parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751380c-4b4e-4429-9b5f-616b4d42a4b4",
   "metadata": {},
   "source": [
    "The Feynman diagram of the model we are testing for collider experiments is below. The mediator is termed as $V$ ($A-V$ represents the axial vector mediator), quarks are denoted by $q$ and gluons by $g$, while the Dark Matter particle is called $\\chi$.\n",
    "\n",
    "The relevant parameters for this model are:\n",
    "   * The coupling between quarks and the mediator $g_q$\n",
    "   * The coupling between DM and the mediator $g_{DM}$\n",
    "   * The mediator mass $m_V$\n",
    "   * The dark matter mass $m_{DM}$\n",
    "\n",
    "The model also allows for a single universal coupling strength $g_l$ for all mediator-lepton interactions, including neutrinos (this is not depicted in the diagrams below). No mixing with the Z-boson is included in this model.\n",
    "\n",
    "While this is a _simplified model_, meaning that not all of its features are fully developed at all scales, portraying the sensitivities of different experiments using a common benchmark is still useful to understand how DM would manifest across experiments in similar classes of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfaec3-3e6f-4300-abae-bd8827cc18a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename = \"img/Feynman.png\", width=600, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa880c-7d93-41fe-8743-f1b9a3aed38c",
   "metadata": {},
   "source": [
    "## Searches at colliders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82aa9a-0301-4625-a667-40a8702a7dbf",
   "metadata": {},
   "source": [
    "Due to the presence of both SM and DM couplings, the main search avenues for this kind of models at colliders are (a, left-hand side diagram) by looking for localized (resonant) excesses signaling the presence of a mediator decaying into pairs of DM particles and (b, right-hand side diagram) by looking for an excess of missing energy (MET) from the mediator decaying to DM particles.  \n",
    "\n",
    "The inputs received from the collider community in the context of Snowmass 2021 and used to prepare the plots in this notebook are: \n",
    "   * MET+jet search (also called monojet search) using the ATLAS detector at the upgraded LHC (HL-LHC) [hllhc-monojet];\n",
    "   * MET+jet and MET+hadronically decaying vector boson at the Future Hadron Collider in the Future Circular Collider (FCC) complex [Harris:2015kda];\n",
    "   * Dijet resonance search at the HL-LHC and at the Future Hadron Collider [Harris:2022kls];\n",
    "   * Dilepton resonance search at the HL-LHC using the ATLAS detector [ATL-PHYS-PUB-2018-044]. The CMS version of this search is also available at [CMS-PAS-FTR-21-005]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefb4a1-27ca-4ad8-aca7-6941eac1361d",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df6cd8e-0737-46fd-ae3a-206da4c2c9bb",
   "metadata": {},
   "source": [
    "[Explain that we need more than one experiment to make a discovery] \n",
    "\n",
    "So far, the presentation of LHC and future hadron collider results has focused on four benchmark scenarios with specific coupling values within these simplified models - in particular, one of the chosen benchmarks (also used in the 2019 European Strategy Briefing Book [4]) use g_q = 0.25, g_l = 0 and g_DM = 1.0. [The following is a WIP] In this work, we highlight the extensions of this benchmark scenario to arbitrary couplings and show how the areas of complementarity (where both direct detection and collider experiments could share information about a discovery) change under different coupling scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c39c4-de2d-4791-872c-eb846bfd56a9",
   "metadata": {},
   "source": [
    "## Notebook for overlaying the sensitivity of future collider and direct detection experiments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbdf86b-ed4b-4519-87a0-79ad9e52055c",
   "metadata": {},
   "source": [
    "### Common imports and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35a6bb-d523-408e-b812-810c6cc66b17",
   "metadata": {},
   "source": [
    "Assorted imports, including a helper script to collect and display direct detection curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b31df7-44e0-4436-8449-550b54ba180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from shapely.geometry import Polygon as shapely_pol\n",
    "import pickle\n",
    "from basic_plotter import *\n",
    "import ROOT\n",
    "import itertools as it\n",
    "import sys, os\n",
    "sys.path.insert(1, '../inputs/directdetection')\n",
    "import collect_dd\n",
    "\n",
    "FULL_SPECTRA_PATH = \"data/Full_spectra\"  # arXiv:[2109.03116]\n",
    "OUTPUT_FOLDER = \"plots\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449f390-f8e9-4bb5-b23c-11316029dd1b",
   "metadata": {},
   "source": [
    "Some more helper functions (TODO: move outside this notebook at some point...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24dc58-db07-4c51-a445-a47be8d73d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_vertical(x1,y1,x2,y2,n) :\n",
    "    ov = sorted([[y1,x1],[y2,x2]])\n",
    "    new_ys = np.linspace(ov[0][0], ov[1][0], num=n)\n",
    "    new_xs = np.interp(new_ys,[ov[0][0],ov[1][0]],[ov[0][1],ov[1][1]])\n",
    "    vertices = list(zip(new_xs,new_ys))\n",
    "    return vertices if y1 < y2 else list(reversed(vertices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e43118-9a91-4192-8904-fca165f8c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect direct detection contours for comparison plots.\n",
    "def get_dd_lines(lineinfo) :\n",
    "    legend_lines = list(lineinfo.keys())\n",
    "    dd_lines = []\n",
    "    for name in legend_lines :\n",
    "        dd_lines.append(lineinfo[name])\n",
    "    return legend_lines, dd_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2058db-e65b-400b-beec-2cdc6ee1a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise(iterable):\n",
    "    # pairwise('ABCDEFG') --> AB BC CD DE EF FG\n",
    "    a, b = it.tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c2dd8a-608e-4e05-b4e5-1faca8f25d89",
   "metadata": {},
   "source": [
    "### Plotting direct detection curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe4278-723b-41b2-8c2c-f5d6e4562aea",
   "metadata": {},
   "source": [
    "#### Plotting one example curve by itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da71be0",
   "metadata": {},
   "source": [
    "Here we plot the Darkside example plot as an example of direct detection. WIP Leo: overlaying other future experiment projections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Define filenames for the full spectra\n",
    "DS200TY_FULL_SPECTRUM_FILE = \"DarkSide-200ty.txt\"  # DS-20k 200 ton-year projection\n",
    "\n",
    "# Load DarkSide-200ty full spectrum\n",
    "try:\n",
    "    # First try with comma delimiter\n",
    "    try:\n",
    "        ds200ty_mass, ds200ty_xsec = np.loadtxt(os.path.join(FULL_SPECTRA_PATH, DS200TY_FULL_SPECTRUM_FILE), \n",
    "                                          delimiter=',', unpack=True)\n",
    "    except:\n",
    "        # If comma fails, try tab delimiter\n",
    "        ds200ty_mass, ds200ty_xsec = np.loadtxt(os.path.join(FULL_SPECTRA_PATH, DS200TY_FULL_SPECTRUM_FILE), \n",
    "                                          delimiter='\\t', unpack=True)\n",
    "    \n",
    "    # Plot DS-200ty data\n",
    "    plt.loglog(ds200ty_mass, ds200ty_xsec, '-', color='red', linewidth=2.5, \n",
    "             label='DarkSide-20k (200 ton-year)')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading DarkSide-200ty full spectrum: {e}\")\n",
    "\n",
    "# Set plot properties\n",
    "plt.xlabel('WIMP Mass [GeV/c²]', fontsize=12)\n",
    "plt.ylabel('WIMP-nucleon σ$_{SI}$ [cm²]', fontsize=12)\n",
    "plt.title('Exclusion 90% C.L.', fontsize=14)\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "# Set axis limitsm\n",
    "plt.xlim(1, 100000)  \n",
    "plt.ylim(1e-48, 1e-39) \n",
    "\n",
    "# Save plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_FOLDER, \"full_spectra_comparison.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd1181-5c97-4e3a-badd-4c597fa9f024",
   "metadata": {},
   "source": [
    "#### Preparing to plot multiple DD curves overlaid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507b153-fc5b-4b88-a8b9-772dd6f67b48",
   "metadata": {},
   "source": [
    "Here we pick up the different constraints in terms of direct detection DM-nucleon interactions, spin-dependent for proton and neutron separately and spin-independent, and have a helper function that associates legends to curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00189ae-edc9-42ef-90f0-41becedc3155",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_proton = collect_dd.get_sd_proton()\n",
    "sd_neutron = collect_dd.get_sd_neutron()\n",
    "spin_independent = collect_dd.get_spin_independent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41556da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spin_independent.update({'Darkside' : [np.array(ds200ty_mass),np.array(ds200ty_xsec)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999298c5-0d07-408a-abe3-f9032801be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect direct detection contours for comparison plots.\n",
    "def get_dd_lines(lineinfo) :\n",
    "    legend_lines = list(lineinfo.keys())\n",
    "    dd_lines = []\n",
    "    for name in legend_lines :\n",
    "        dd_lines.append(lineinfo[name])\n",
    "    return legend_lines, dd_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db33ec65-6248-42ca-87b6-2b9dd4e0a10e",
   "metadata": {},
   "source": [
    "### Plotting collider curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e0cef-47dc-48ea-b0a0-6f8b6c440cc5",
   "metadata": {},
   "source": [
    "We will now test a few couplings of the mediator to quarks (gq), leptons (gl) and dark matter (test_gdm), for the static version of the plots. The choice of these couplings does not change direct detection results, but they change the sensitivity of the collider results as they influence both production and decay of the mediator in a collider.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845c13a-4946-47f2-90e6-c4e4a3989b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gq = [0.25]\n",
    "test_gdm = [1.0]\n",
    "test_gl = [0.00]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd3f811-4e48-4ed8-bc3c-6843bfdd0549",
   "metadata": {},
   "source": [
    "These two following functions are used to convert benchmark points that are constrained in collider space, identified by the couplings, the mediator mass and the DM mass, into benchmark points in the direct detection space of the nucleon-DM spin-dependent and spin-independent cross-section (equation 4.10 and 4.3 of [2]). \n",
    "Note: these conversion formulas do not include effects due to the difference in energy scales of the two experiments [3], but they do not change the picture for the purpose of these plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d042e2e-34ae-4947-a40b-0b09c47f00db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eqn 4.10 https://arxiv.org/pdf/1603.04156.pdf#page12\n",
    "def calculate_sd(gq, gdm, gl, mMed, mdm) :\n",
    "\n",
    "    mn = 0.939 # GeV\n",
    "    val = 2.4e-42 * (gq*gdm/0.25)**2 * (1000./mMed)**4 * (mn*mdm/(mn+mdm))**2\n",
    "    return val\n",
    "\n",
    "# eqn 4.3: https://arxiv.org/pdf/1603.04156.pdf#page12\n",
    "def calculate_si(gq, gdm, gl, mMed, mdm) :\n",
    "\n",
    "    mn = 0.939 # GeV\n",
    "    val = 6.9e-41 * (gq*gdm/0.25)**2 * (1000./mMed)**4 * (mn*mdm/(mn+mdm))**2\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b47e8a-0be2-4814-8331-323d524bdf07",
   "metadata": {},
   "source": [
    "This function takes care of the plotting given inputs in terms of collider name, kind of model (vector or axial vector mediator), contour lines, legend lines, and what couplings are fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b8bec-b4c5-4070-8603-6386b93e3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(collider, model, contours, legend_lines, fix_couplings, extra_tag = \"\") :\n",
    "    xlow = 1\n",
    "    xhigh = 2000 if 'hl-lhc' in collider else 4000\n",
    "    if 'vector' in model : ylow = 1e-48 if 'hl-lhc' in collider else 1e-50\n",
    "    else : ylow = 1e-46 if 'hl-lhc' in collider else 1e-50\n",
    "    yhigh = 1e-37 if 'hl-lhc' in collider else 1e-42    \n",
    "    usepath = \"plots/directdetection/\"+collider\n",
    "    formatters = {\"gq\" : \"q\", \"gdm\" : \"\\chi\", \"gl\" : \"l\"}\n",
    "    treat_as_scaling = False\n",
    "    # At least 2 fixed couplings.\n",
    "    if len(fix_couplings.keys()) > 2 :\n",
    "        label_line =  \"{0}\\n{7}, g$_{5}$={2}\\ng$_{4}$={1}, g$_{6}$={3}\".format((\"Axial-vector\" if 'axial' in model else \"Vector\"),fix_couplings[\"gq\"],fix_couplings[\"gdm\"],fix_couplings[\"gl\"],\"q\",\"\\chi\",\"l\",collider.upper())\n",
    "        tag_line = model+\"_gq{0}_gdm{1}_gl{2}\".format(fix_couplings[\"gq\"],fix_couplings[\"gdm\"],fix_couplings[\"gl\"])\n",
    "    else :\n",
    "        treat_as_scaling = True\n",
    "        usecouplings = list(fix_couplings.keys())\n",
    "        useformats = []\n",
    "        vals = []\n",
    "        for coupling in usecouplings :\n",
    "            useformats.append(formatters[coupling])\n",
    "            vals.append(fix_couplings[coupling])\n",
    "        label_line = \"{0}, {5}\\ng$_{3}$={1}, g$_{4}$={2}\".format((\"Axial-vector\" if 'axial' in model else \"Vector\"),vals[0],vals[1],useformats[0],useformats[1],collider.upper())\n",
    "        tag_line = model+\"_{0}{1}_{2}{3}\".format(usecouplings[0],vals[0],usecouplings[1],vals[1])\n",
    "    if extra_tag : tag_line = tag_line + \"_\" + extra_tag\n",
    "    # And draw. First, version without DD experiment lines\n",
    "    # Then draw the plots with DD lines on\n",
    "    if 'vector' in model :\n",
    "        # If doing fcc-hh, remove MIGD line which is too high up to be seen.\n",
    "        formatted_lines = get_dd_lines(spin_independent)\n",
    "        ddcurves = []\n",
    "        ddnames = []\n",
    "        for name, line in zip(formatted_lines[0],formatted_lines[1]) :\n",
    "            if not ('fcc' in collider and 'MIGD' in name) :\n",
    "                ddcurves.append(line)\n",
    "                ddnames.append(name)\n",
    "        use_ylabel = \"$\\sigma_{SI}$ ($\\chi$-nucleon) [cm$^2$]\"\n",
    "        drawDDPlot(contours,legend_lines, this_tag = tag_line, plot_path = usepath, addText = label_line, ylabel=use_ylabel, is_scaling=treat_as_scaling, transluscent=treat_as_scaling, xhigh=xhigh, ylow=ylow, yhigh=yhigh)\n",
    "        drawDDPlot(contours,legend_lines, this_tag = tag_line+\"_withDD\", plot_path = usepath, addText = label_line, ylabel=use_ylabel, is_scaling=treat_as_scaling, transluscent=treat_as_scaling, xhigh=xhigh, ylow=ylow, yhigh=yhigh, dd_curves = ddcurves, dd_legendlines = ddnames)\n",
    "    else :\n",
    "        use_ylabel = \"$\\sigma_{SD}$ ($\\chi$-nucleon) [cm$^2$]\"\n",
    "        drawDDPlot(contours,legend_lines, this_tag = tag_line, plot_path = usepath, addText = label_line, ylabel=use_ylabel, is_scaling=treat_as_scaling, transluscent=treat_as_scaling, xhigh=xhigh, ylow=ylow, yhigh=yhigh)\n",
    "        formatted_lines_p = get_dd_lines(sd_proton)\n",
    "        use_ylabel = \"$\\sigma_{SD}$ ($\\chi$-proton) [cm$^2$]\"\n",
    "        drawDDPlot(contours,legend_lines, this_tag = tag_line+\"_withDD_proton\", plot_path = usepath, addText = label_line, ylabel=use_ylabel, is_scaling=treat_as_scaling, transluscent=treat_as_scaling, xhigh=xhigh, ylow=ylow, yhigh=yhigh, dd_curves = formatted_lines_p[1], dd_legendlines = formatted_lines_p[0])  \n",
    "        formatted_lines_n = get_dd_lines(sd_neutron)\n",
    "        use_ylabel = \"$\\sigma_{SD}$ ($\\chi$-neutron) [cm$^2$]\"\n",
    "        drawDDPlot(contours,legend_lines, this_tag = tag_line+\"_withDD_neutron\", plot_path = usepath, addText = label_line, ylabel=use_ylabel, is_scaling=treat_as_scaling, transluscent=treat_as_scaling, xhigh=xhigh, ylow=ylow, yhigh=yhigh, dd_curves = formatted_lines_n[1], dd_legendlines = formatted_lines_n[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b7b6dd-3b7b-4c6d-9b20-4a434482e52b",
   "metadata": {},
   "source": [
    "Here we load all the files including polygons for the HL-LHC vector mediated DM benchmark models, and make different grids of plots with different options using python dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81e93a-aa5a-4bba-8cbf-4a46e4ea8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_signature_exclusion(loaded_polygons, gq, gdm, gl, model):\n",
    "    contours_list = []\n",
    "    legend_lines = []\n",
    "    exclusions_dd = {}\n",
    "    exclusions_separate_dd = {}\n",
    "\n",
    "    for signature in ['dijet', 'monojet', 'dilepton']:\n",
    "        key = (gq, gdm, gl)\n",
    "        if key not in loaded_polygons[signature]:\n",
    "            continue\n",
    "        exclusions = loaded_polygons[signature][key]\n",
    "        if not exclusions:\n",
    "            continue\n",
    "\n",
    "        inner_contours = []\n",
    "        for contour in exclusions:\n",
    "            original_vertices = list(contour.exterior.coords)\n",
    "            use_vertices = []\n",
    "            # Reconnect loop by adding first one back\n",
    "            for (x, y), (xnext, ynext) in pairwise(original_vertices + [original_vertices[0]]):\n",
    "                use_vertices.append((x, y))\n",
    "                if (y > 0 or ynext > 0) and ynext < 50:\n",
    "                    new_vertices = interpolate_vertical(x, y, xnext, ynext, 100)\n",
    "                    use_vertices += new_vertices\n",
    "            contour_DD_raw = []\n",
    "            raw_graph = ROOT.TGraph()\n",
    "            for (x, y) in use_vertices:\n",
    "                sigma = calculate_sd(gq, gdm, gl, x, y) if model == 'axial' else calculate_si(gq, gdm, gl, x, y)\n",
    "                contour_DD_raw.append((y, sigma))\n",
    "                raw_graph.AddPoint(y, sigma)\n",
    "            contour_DD = shapely_pol(contour_DD_raw)\n",
    "            inner_contours.append(contour_DD)\n",
    "        \n",
    "        contours_list.append(inner_contours)\n",
    "        legend_lines.append(signature.capitalize())\n",
    "        exclusions_dd[signature] = {key: contours_list}\n",
    "        exclusions_separate_dd[signature] = {key: inner_contours}\n",
    "\n",
    "    return contours_list, legend_lines, exclusions_dd, exclusions_separate_dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56407628-535f-41ca-8ae3-29fba21dae94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df634b7-ab7b-4e15-9e5b-089d90bf09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_couplingscan_plots(collider, model, param_loop, fixed_params, exclusions_dd, param_name):\n",
    "    contours_list_couplingscan = []\n",
    "    legend_lines_couplingscan = []\n",
    "\n",
    "    for param_value in param_loop:\n",
    "        contours_list = []\n",
    "        for signature in ['dijet', 'monojet', 'dilepton']:\n",
    "            key = tuple(fixed_params.get(k, param_value) for k in ['gq', 'gdm', 'gl'])\n",
    "            if key not in exclusions_dd[signature]:\n",
    "                continue\n",
    "            contours_list += exclusions_dd[signature][key]\n",
    "        full_polygons = merge_exclusions(contours_list)\n",
    "        if not full_polygons:\n",
    "            continue\n",
    "        contours_list_couplingscan.append(full_polygons)\n",
    "        legend_lines_couplingscan.append(f\"g$_{{{param_name}}}$={param_value}\")\n",
    "    \n",
    "    make_plots(collider, model, contours_list_couplingscan, legend_lines_couplingscan, fixed_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2e93a-f496-4081-80ff-2736ebdc1293",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5abf52a-57ec-43fa-b517-a5530383c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_signature_plots(collider, model, test_loop, fixed_params, exclusions_separate_dd, param_name, signature):\n",
    "    sub_contours_list = []\n",
    "    sub_legends_list = []\n",
    "\n",
    "    for param_value in test_loop:\n",
    "        key = tuple(fixed_params.get(k, param_value) for k in ['gq', 'gdm', 'gl'])\n",
    "        if key not in exclusions_separate_dd[signature]:\n",
    "            continue\n",
    "        sub_contours_list.append(exclusions_separate_dd[signature][key])\n",
    "        sub_legends_list.append(f\"g$_{{{param_name}}}$={param_value}\")\n",
    "\n",
    "    make_plots(collider, model, sub_contours_list, sub_legends_list, fixed_params, extra_tag=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11850983-0f07-41dc-bee4-0cd31e261b31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569cb0b9-5383-444b-a64c-75c5107fa17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "for collider in ['hl-lhc'] :#, 'fcc-hh'] :\n",
    "    for model in ['vector'] : #,'axial'] :\n",
    "\n",
    "        # Limits with fixed couplings\n",
    "        # Load pickle files with polygons\n",
    "        with open(f'{model}_exclusion_contours_{collider}.pkl', \"rb\") as poly_file:\n",
    "            loaded_polygons = pickle.load(poly_file)\n",
    "\n",
    "        # Grid of plots:\n",
    "        exclusions_dd = {'dijet': {}, 'monojet': {}, 'dilepton': {}}\n",
    "        exclusions_separate_dd = {'dijet': {}, 'monojet': {}, 'dilepton': {}}\n",
    "\n",
    "        for gdm in test_gdm:\n",
    "            for gq in test_gq:\n",
    "                for gl in test_gl:\n",
    "                    contours_list, legend_lines, excl_dd, excl_sep_dd = process_single_signature_exclusion(\n",
    "                        loaded_polygons, gq, gdm, gl, model\n",
    "                    )\n",
    "\n",
    "                    for sig in exclusions_dd:\n",
    "                        exclusions_dd[sig].update(excl_dd.get(sig, {}))\n",
    "                        exclusions_separate_dd[sig].update(excl_sep_dd.get(sig, {}))\n",
    "\n",
    "                    make_plots(collider, model, contours_list, legend_lines, {\"gq\": gq, \"gdm\": gdm, \"gl\": gl})\n",
    "\n",
    "                make_couplingscan_plots(\n",
    "                    collider, model, test_gl, {\"gq\": gq, \"gdm\": gdm}, exclusions_dd, \"l\"\n",
    "                )\n",
    "\n",
    "                for gl in test_gl:\n",
    "                    make_couplingscan_plots(\n",
    "                        collider, model, test_gq, {\"gl\": gl, \"gdm\": gdm}, exclusions_dd, \"q\"\n",
    "                    )\n",
    "                    for signature in ['dijet', 'monojet', 'dilepton']:\n",
    "                        overlay_signature_plots(\n",
    "                            collider, model, test_gq, {\"gl\": gl, \"gdm\": gdm},\n",
    "                            exclusions_separate_dd, \"q\", signature\n",
    "                        )\n",
    "\n",
    "        for gq in test_gq:\n",
    "            for gl in test_gl:\n",
    "                make_couplingscan_plots(\n",
    "                    collider, model, test_gdm, {\"gq\": gq, \"gl\": gl}, exclusions_dd, \"\\\\chi\"\n",
    "                )\n",
    "                for signature in ['dijet', 'monojet', 'dilepton']:\n",
    "                    overlay_signature_plots(\n",
    "                        collider, model, test_gdm, {\"gq\": gq, \"gl\": gl},\n",
    "                        exclusions_separate_dd, \"\\\\chi\", signature\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f00f36-b5d7-4811-90ea-eb65152d7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle files with polygons\n",
    "for collider in ['hl-lhc'] :#, 'fcc-hh'] :\n",
    "    for model in ['vector'] : #,'axial'] :\n",
    "\n",
    "        # Limits with fixed couplings\n",
    "        with open('{0}_exclusion_contours_{1}.pkl'.format(model,collider), \"rb\") as poly_file:\n",
    "            loaded_polygons = pickle.load(poly_file)\n",
    "\n",
    "            # Grid of plots:\n",
    "            exclusions_dd = {'dijet' : {},'monojet' : {},'dilepton' : {}}\n",
    "            exclusions_separate_dd = {'dijet' : {},'monojet' : {},'dilepton' : {}}\n",
    "            for gdm in test_gdm :\n",
    "                for gq in test_gq :\n",
    "                    contours_list_couplingscan = []\n",
    "                    legend_lines_couplingscan = []                \n",
    "                    for gl in test_gl :\n",
    "                        contours_list = []\n",
    "                        legend_lines = []\n",
    "                        for signature in ['dijet','monojet','dilepton'] :\n",
    "                            # e.g. no coupling to leptons, skip:\n",
    "                            if (gq, gdm, gl) not in loaded_polygons[signature].keys() :\n",
    "                                continue\n",
    "                            exclusions = loaded_polygons[signature][(gq, gdm, gl)]\n",
    "                            if not exclusions : continue\n",
    "                            inner_contours = []\n",
    "                            for contour in exclusions :\n",
    "                                original_vertices = list(contour.exterior.coords)\n",
    "                                use_vertices = []\n",
    "                                # Reconnect loop by adding first one back\n",
    "                                for (x, y), (xnext,ynext) in pairwise(original_vertices+[original_vertices[0]]):\n",
    "                                    use_vertices.append((x, y))\n",
    "                                    if (y > 0 or ynext > 0) and ynext < 50 :\n",
    "                                        new_vertices = interpolate_vertical(x,y,xnext,ynext,100)\n",
    "                                        use_vertices = use_vertices + new_vertices\n",
    "                                contour_DD_raw = []\n",
    "                                raw_graph = ROOT.TGraph()\n",
    "                                for (x, y) in use_vertices :\n",
    "                                    if model=='axial' :\n",
    "                                        sigma = calculate_sd(gq, gdm, gl, x, y)\n",
    "                                    else :\n",
    "                                        sigma = calculate_si(gq, gdm, gl, x, y)\n",
    "                                    contour_DD_raw.append((y, sigma))\n",
    "                                    raw_graph.AddPoint(y, sigma)\n",
    "                                contour_DD = shapely_pol(contour_DD_raw)\n",
    "                                inner_contours.append(contour_DD)\n",
    "                            contours_list.append(inner_contours)\n",
    "                            legend_lines.append(signature.capitalize())\n",
    "                            exclusions_dd[signature][(gq, gdm, gl)] = contours_list\n",
    "                            exclusions_separate_dd[signature][(gq, gdm, gl)] = inner_contours\n",
    "                        # First set of plots: 3 contours, one plot for every coupling combo\n",
    "                        make_plots(collider, model, contours_list, legend_lines, {\"gq\" : gq, \"gdm\" : gdm, \"gl\" : gl})\n",
    "                        full_polygons = merge_exclusions(contours_list)\n",
    "                        if not full_polygons : continue \n",
    "                        contours_list_couplingscan.append(full_polygons)\n",
    "                        legend_lines_couplingscan.append(\"g$_{0}$={1}\".format(\"l\",gl))\n",
    "                    # Second set of plots: merge all contours; fix gq and vary gl.\n",
    "                    # Note this is not meaningful where we don't have dilepton projections - skip then.\n",
    "                    make_plots(collider, model, contours_list_couplingscan, legend_lines_couplingscan, {\"gq\" : gq, \"gdm\" : gdm})\n",
    "                    # Could do signature only, fixed gq and varying gl, \n",
    "                    # but don't think we need it right now. Would go here.\n",
    "\n",
    "                # Need second set of plots with gl fixed instead:\n",
    "                for gl in test_gl :\n",
    "                    contours_list_couplingscan = []\n",
    "                    legend_lines_couplingscan = []\n",
    "                    for gq in test_gq :\n",
    "                        contours_list = []\n",
    "                        for signature in ['dijet','monojet','dilepton'] :\n",
    "                            # e.g. no coupling to leptons, skip:\n",
    "                            if (gq, gdm, gl) not in exclusions_dd[signature].keys() :\n",
    "                                continue\n",
    "                            exclusions = exclusions_dd[signature][(gq, gdm, gl)]\n",
    "                            contours_list+=exclusions\n",
    "                        full_polygons = merge_exclusions(contours_list)\n",
    "                        if not full_polygons : continue\n",
    "                        contours_list_couplingscan.append(full_polygons)\n",
    "                        legend_lines_couplingscan.append(\"g$_{0}$={1}\".format(\"q\",gq))\n",
    "                    make_plots(collider, model, contours_list_couplingscan, legend_lines_couplingscan, {\"gl\" : gl, \"gdm\" : gdm})\n",
    "                \n",
    "                    # A version overlaying all monojet and overlaying all dijet, but not combining\n",
    "                    for signature in ['dijet','monojet','dilepton'] :\n",
    "                        sub_contours_list = []\n",
    "                        sub_legends_list = []\n",
    "                        for gq in test_gq :\n",
    "                            if (gq, gdm, gl) not in exclusions_separate_dd[signature].keys() : continue\n",
    "                            sub_contours_list.append(exclusions_separate_dd[signature][(gq, gdm, gl)])\n",
    "                            sub_legends_list.append(\"g$_{0}$={1}\".format(\"q\",gq))\n",
    "                        make_plots(collider, model, sub_contours_list, sub_legends_list, {\"gl\" : gl, \"gdm\" : gdm}, extra_tag = signature)\n",
    "            # And now third set of plots with gq and gl fixed:\n",
    "            for gq in test_gq :\n",
    "                for gl in test_gl :\n",
    "                    contours_list_couplingscan = []\n",
    "                    legend_lines_couplingscan = []\n",
    "                    for gdm in test_gdm :\n",
    "                        contours_list = []\n",
    "                        for signature in ['dijet','monojet','dilepton'] :\n",
    "                            # e.g. no coupling to leptons, skip:\n",
    "                            if (gq, gdm, gl) not in exclusions_dd[signature].keys() :\n",
    "                                continue                            \n",
    "                            exclusions = exclusions_dd[signature][(gq, gdm, gl)]\n",
    "                            contours_list+=exclusions\n",
    "                        if all(not i for i in contours_list) : continue\n",
    "                        full_polygons = merge_exclusions(contours_list)\n",
    "                        contours_list_couplingscan.append(full_polygons)\n",
    "                        legend_lines_couplingscan.append(\"g$_{0}$={1}\".format(\"\\chi\",gdm))\n",
    "                    make_plots(collider, model, contours_list_couplingscan, legend_lines_couplingscan, {\"gq\" : gq, \"gl\" : gl})\n",
    "                    # A version overlaying all monojet and overlaying all dijet, but not combining\n",
    "                    for signature in ['dijet','monojet','dilepton'] :\n",
    "                        sub_contours_list = []\n",
    "                        sub_legends_list = []\n",
    "                        for gdm in test_gdm :\n",
    "                            if (gq, gdm, gl) not in exclusions_separate_dd[signature].keys() : continue\n",
    "                            sub_contours_list.append(exclusions_separate_dd[signature][(gq, gdm, gl)])\n",
    "                            sub_legends_list.append(\"g$_{0}$={1}\".format(\"\\chi\",gdm))                   \n",
    "                        make_plots(collider, model, sub_contours_list,sub_legends_list, {\"gq\" : gq, \"gl\" : gl}, extra_tag = signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80e21d-41b3-4f51-a08a-8382cad4de68",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[1] A. Boveia et al., Dark Matter Benchmark Models for Early LHC Run-2 Searches: Report of the ATLAS/CMS Dark Matter Forum, https://doi.org/10.1016/j.dark.2019.100371 \n",
    "\n",
    "[2] https://arxiv.org/pdf/2203.12035\n",
    "\n",
    "[3] \n",
    "\n",
    "[4] https://arxiv.org/pdf/1603.04156#page12\n",
    "\n",
    "[5] https://arxiv.org/abs/1411.3342\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017870df-398b-468e-a67c-bedf9749c691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
